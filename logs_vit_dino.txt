GPU
Epoch 1: Batch 0: Loss 1.3403449058532715, R2 -0.17012706398963928
Epoch 1: Batch 100: Loss 1.0338577032089233, R2 0.04786859080195427
Epoch 1: Batch 200: Loss 0.8306156992912292, R2 0.10027919709682465
Epoch 1: Batch 300: Loss 0.6916340589523315, R2 0.13022123277187347
Epoch 1: Batch 400: Loss 0.8932117223739624, R2 0.14746028184890747
Epoch 1: Batch 500: Loss 0.8998769521713257, R2 0.1593591868877411
Training Loss 0.8360790516837496 R2 0.1638699322938919
Val Loss: 0.7624168233836398. R2: 0.2502810060977936
Epoch 2: Batch 0: Loss 0.9018775820732117, R2 0.309189110994339
Epoch 2: Batch 100: Loss 1.0805883407592773, R2 0.22921860218048096
Epoch 2: Batch 200: Loss 0.8069035410881042, R2 0.22992977499961853
Epoch 2: Batch 300: Loss 0.6106364727020264, R2 0.22589191794395447
Epoch 2: Batch 400: Loss 0.7412704229354858, R2 0.22823695838451385
Epoch 2: Batch 500: Loss 0.8812001943588257, R2 0.228180930018425
Training Loss 0.77184311374747 R2 0.22811532020568848
Val Loss: 0.7473283414455021. R2: 0.2649533748626709
Epoch 3: Batch 0: Loss 0.9373229742050171, R2 0.2712170481681824
Epoch 3: Batch 100: Loss 0.6395066976547241, R2 0.23974227905273438
Epoch 3: Batch 200: Loss 0.7532098293304443, R2 0.24894925951957703
Epoch 3: Batch 300: Loss 0.565942645072937, R2 0.24663862586021423
Epoch 3: Batch 400: Loss 0.9270737171173096, R2 0.247263103723526
Epoch 3: Batch 500: Loss 0.5052626132965088, R2 0.2457742542028427
Training Loss 0.7539380387956366 R2 0.24605849385261536
Val Loss: 0.7381371592774111. R2: 0.2739780843257904
Epoch 4: Batch 0: Loss 0.8509979248046875, R2 0.2823556661605835
Epoch 4: Batch 100: Loss 1.0127991437911987, R2 0.26189979910850525
Epoch 4: Batch 200: Loss 0.8634428977966309, R2 0.25871220231056213
Epoch 4: Batch 300: Loss 0.7757253646850586, R2 0.25798746943473816
Epoch 4: Batch 400: Loss 0.673189640045166, R2 0.25622695684432983
Epoch 4: Batch 500: Loss 1.0078728199005127, R2 0.25884562730789185
Training Loss 0.7409989833281929 R2 0.25898832082748413
Val Loss: 0.7338855948080035. R2: 0.2781379818916321
Epoch 5: Batch 0: Loss 0.9510903358459473, R2 0.31422147154808044
Epoch 5: Batch 100: Loss 0.8820236921310425, R2 0.2707115113735199
Epoch 5: Batch 200: Loss 0.6223813891410828, R2 0.2706325650215149
Epoch 5: Batch 300: Loss 0.7427501678466797, R2 0.26780709624290466
Epoch 5: Batch 400: Loss 0.8819998502731323, R2 0.26550108194351196
Epoch 5: Batch 500: Loss 0.7944731712341309, R2 0.2671217620372772
Training Loss 0.7322037492730961 R2 0.2678103446960449
Val Loss: 0.7257849297979299. R2: 0.2860451340675354
Epoch 6: Batch 0: Loss 0.6338768601417542, R2 0.36345112323760986
Epoch 6: Batch 100: Loss 0.785996675491333, R2 0.27845436334609985
Epoch 6: Batch 200: Loss 0.8826424479484558, R2 0.27472805976867676
Epoch 6: Batch 300: Loss 0.6360893249511719, R2 0.275993287563324
Epoch 6: Batch 400: Loss 0.7495075464248657, R2 0.2743632197380066
Epoch 6: Batch 500: Loss 0.6498637199401855, R2 0.2741623520851135
Training Loss 0.7255907486505614 R2 0.27427351474761963
Val Loss: 0.723258880131385. R2: 0.2884962856769562
Epoch 7: Batch 0: Loss 0.874962568283081, R2 0.2024500072002411
Epoch 7: Batch 100: Loss 0.8062180280685425, R2 0.29254311323165894
Epoch 7: Batch 200: Loss 1.0477358102798462, R2 0.29023441672325134
Epoch 7: Batch 300: Loss 0.7644318342208862, R2 0.2920551300048828
Epoch 7: Batch 400: Loss 0.7995342016220093, R2 0.2953697144985199
Epoch 7: Batch 500: Loss 0.5592250823974609, R2 0.2940739393234253
Training Loss 0.704514438265804 R2 0.2954586148262024
Val Loss: 0.7187049309996998. R2: 0.2929942011833191
Epoch 8: Batch 0: Loss 0.660520076751709, R2 0.20948800444602966
Epoch 8: Batch 100: Loss 0.6542297601699829, R2 0.3047124743461609
Epoch 8: Batch 200: Loss 0.5625004768371582, R2 0.3072739243507385
Epoch 8: Batch 300: Loss 0.598263680934906, R2 0.3024739623069763
Epoch 8: Batch 400: Loss 0.7653324007987976, R2 0.3029954433441162
Epoch 8: Batch 500: Loss 0.9322313070297241, R2 0.30313876271247864
Training Loss 0.6977989814378238 R2 0.30219709873199463
Val Loss: 0.7187324216698899. R2: 0.29297783970832825
Epoch 9: Batch 0: Loss 0.5612072348594666, R2 0.27741125226020813
Epoch 9: Batch 100: Loss 0.9351670145988464, R2 0.3078130781650543
Epoch 9: Batch 200: Loss 0.8347598910331726, R2 0.3041544556617737
Epoch 9: Batch 300: Loss 0.49404001235961914, R2 0.3024784028530121
Epoch 9: Batch 400: Loss 0.504295825958252, R2 0.30056333541870117
Epoch 9: Batch 500: Loss 0.5131155848503113, R2 0.3026003837585449
Training Loss 0.6987029269502611 R2 0.3013126254081726
Val Loss: 0.7190028028014828. R2: 0.2927052974700928
Epoch 10: Batch 0: Loss 0.7905124425888062, R2 0.3400968313217163
Epoch 10: Batch 100: Loss 0.7224305868148804, R2 0.31618359684944153
Epoch 10: Batch 200: Loss 0.6270912885665894, R2 0.310894250869751
Epoch 10: Batch 300: Loss 0.7391983270645142, R2 0.3091717064380646
Epoch 10: Batch 400: Loss 0.5254572629928589, R2 0.3088432848453522
Epoch 10: Batch 500: Loss 0.7201869487762451, R2 0.30795595049858093
Training Loss 0.6930281910289258 R2 0.30696672201156616
Val Loss: 0.7183575571021613. R2: 0.293323278427124
Epoch 11: Batch 0: Loss 0.6911709308624268, R2 0.2277079075574875
Epoch 11: Batch 100: Loss 0.7989187240600586, R2 0.3045760989189148
Epoch 11: Batch 200: Loss 0.6735168695449829, R2 0.30740684270858765
Epoch 11: Batch 300: Loss 0.7864302396774292, R2 0.3034435212612152
Epoch 11: Batch 400: Loss 0.446913480758667, R2 0.3065328299999237
Epoch 11: Batch 500: Loss 0.7525337934494019, R2 0.3084879219532013
Training Loss 0.6895483594209065 R2 0.3104339838027954
Val Loss: 0.7164087865282508. R2: 0.295224130153656
Epoch 12: Batch 0: Loss 0.7086617350578308, R2 0.23182222247123718
Epoch 12: Batch 100: Loss 0.688522219657898, R2 0.3081229329109192
Epoch 12: Batch 200: Loss 0.6785556077957153, R2 0.31227773427963257
Epoch 12: Batch 300: Loss 0.6490510702133179, R2 0.3106193542480469
Epoch 12: Batch 400: Loss 0.7217668294906616, R2 0.3110736608505249
Epoch 12: Batch 500: Loss 0.4926764667034149, R2 0.3109477460384369
Training Loss 0.6901974657144934 R2 0.30970335006713867
Val Loss: 0.7184171021422919. R2: 0.29328423738479614
Epoch 13: Batch 0: Loss 0.8548943996429443, R2 0.3090091347694397
Epoch 13: Batch 100: Loss 0.5288941264152527, R2 0.3195727467536926
Epoch 13: Batch 200: Loss 0.5615960359573364, R2 0.3146161735057831
Epoch 13: Batch 300: Loss 0.5475450158119202, R2 0.31627213954925537
Epoch 13: Batch 400: Loss 0.9806215763092041, R2 0.3160969913005829
Epoch 13: Batch 500: Loss 0.6486963629722595, R2 0.3169442117214203
Training Loss 0.684080938406536 R2 0.3158385753631592
Val Loss: 0.7171600943102556. R2: 0.2945106029510498
Epoch 14: Batch 0: Loss 0.8498412370681763, R2 0.24293994903564453
Epoch 14: Batch 100: Loss 0.6952414512634277, R2 0.3009641468524933
Epoch 14: Batch 200: Loss 0.6789708137512207, R2 0.3072811961174011
Epoch 14: Batch 300: Loss 0.6902346611022949, R2 0.31089845299720764
Epoch 14: Batch 400: Loss 0.740935742855072, R2 0.312629759311676
Epoch 14: Batch 500: Loss 0.7903838157653809, R2 0.31245025992393494
Training Loss 0.6866838199626035 R2 0.3132920265197754
Val Loss: 0.7168906030847746. R2: 0.2947734594345093
Epoch 15: Batch 0: Loss 0.5946882367134094, R2 0.32563459873199463
Epoch 15: Batch 100: Loss 0.7348880767822266, R2 0.3188127875328064
Epoch 15: Batch 200: Loss 0.7004364728927612, R2 0.31581318378448486
Epoch 15: Batch 300: Loss 0.5639869570732117, R2 0.3178945779800415
Epoch 15: Batch 400: Loss 0.8008249998092651, R2 0.3161326050758362
Epoch 15: Batch 500: Loss 0.5679675936698914, R2 0.3152502477169037
Training Loss 0.6857178391015838 R2 0.31405067443847656
Val Loss: 0.716894700027564. R2: 0.2947680652141571
Min Val Loss Epoch 11
Final Arrays
[np.float64(0.8360790516837496), np.float64(0.77184311374747), np.float64(0.7539380387956366), np.float64(0.7409989833281929), np.float64(0.7322037492730961), np.float64(0.7255907486505614), np.float64(0.704514438265804), np.float64(0.6977989814378238), np.float64(0.6987029269502611), np.float64(0.6930281910289258), np.float64(0.6895483594209065), np.float64(0.6901974657144934), np.float64(0.684080938406536), np.float64(0.6866838199626035), np.float64(0.6857178391015838)]
[0.1638699322938919, 0.22811532020568848, 0.24605849385261536, 0.25898832082748413, 0.2678103446960449, 0.27427351474761963, 0.2954586148262024, 0.30219709873199463, 0.3013126254081726, 0.30696672201156616, 0.3104339838027954, 0.30970335006713867, 0.3158385753631592, 0.3132920265197754, 0.31405067443847656]
[np.float64(0.7624168233836398), np.float64(0.7473283414455021), np.float64(0.7381371592774111), np.float64(0.7338855948080035), np.float64(0.7257849297979299), np.float64(0.723258880131385), np.float64(0.7187049309996998), np.float64(0.7187324216698899), np.float64(0.7190028028014828), np.float64(0.7183575571021613), np.float64(0.7164087865282508), np.float64(0.7184171021422919), np.float64(0.7171600943102556), np.float64(0.7168906030847746), np.float64(0.716894700027564)]
[0.2502810060977936, 0.2649533748626709, 0.2739780843257904, 0.2781379818916321, 0.2860451340675354, 0.2884962856769562, 0.2929942011833191, 0.29297783970832825, 0.2927052974700928, 0.293323278427124, 0.295224130153656, 0.29328423738479614, 0.2945106029510498, 0.2947734594345093, 0.2947680652141571]
