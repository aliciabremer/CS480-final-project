GPU
Epoch 1: Batch 0: Loss 1.027117371559143, R2 -0.6938962936401367
Epoch 1: Batch 100: Loss 0.9795454740524292, R2 0.07977577298879623
Epoch 1: Batch 200: Loss 0.9147053956985474, R2 0.12538784742355347
Epoch 1: Batch 300: Loss 0.7342801094055176, R2 0.1462078094482422
Epoch 1: Batch 400: Loss 0.8086110353469849, R2 0.15938717126846313
Epoch 1: Batch 500: Loss 0.763617217540741, R2 0.16595640778541565
Training Loss 0.8312878295284356 R2 0.16870257258415222
Val Loss: 0.770455589189249. R2: 0.24236325919628143
Epoch 2: Batch 0: Loss 0.6312187910079956, R2 -0.0567917637526989
Epoch 2: Batch 100: Loss 0.7290475368499756, R2 0.2170085310935974
Epoch 2: Batch 200: Loss 0.7098207473754883, R2 0.21406279504299164
Epoch 2: Batch 300: Loss 1.0619831085205078, R2 0.2091987133026123
Epoch 2: Batch 400: Loss 0.7264178991317749, R2 0.21037352085113525
Epoch 2: Batch 500: Loss 0.9708783030509949, R2 0.21165966987609863
Training Loss 0.7882880378253345 R2 0.21172642707824707
Val Loss: 0.7809183715897448. R2: 0.23222510516643524
Epoch 3: Batch 0: Loss 0.9127464294433594, R2 0.10187514126300812
Epoch 3: Batch 100: Loss 0.8677462339401245, R2 0.2247399538755417
Epoch 3: Batch 200: Loss 0.7323788404464722, R2 0.22021488845348358
Epoch 3: Batch 300: Loss 0.662691056728363, R2 0.220388263463974
Epoch 3: Batch 400: Loss 0.8598057627677917, R2 0.22226467728614807
Epoch 3: Batch 500: Loss 0.7017192840576172, R2 0.22199928760528564
Training Loss 0.777172893725638 R2 0.2228032797574997
Val Loss: 0.7747903233941864. R2: 0.23847660422325134
Epoch 4: Batch 0: Loss 0.5510863661766052, R2 -0.7751277685165405
Epoch 4: Batch 100: Loss 0.816591203212738, R2 0.21482202410697937
Epoch 4: Batch 200: Loss 0.8642405867576599, R2 0.21771833300590515
Epoch 4: Batch 300: Loss 0.7674058675765991, R2 0.22360825538635254
Epoch 4: Batch 400: Loss 0.7902296781539917, R2 0.2241820991039276
Epoch 4: Batch 500: Loss 0.8383581638336182, R2 0.22539252042770386
Training Loss 0.7743430337232857 R2 0.22566823661327362
Val Loss: 0.7711536577519249. R2: 0.24196025729179382
Epoch 5: Batch 0: Loss 0.8533697128295898, R2 0.19706377387046814
Epoch 5: Batch 100: Loss 0.7271976470947266, R2 0.2464892566204071
Epoch 5: Batch 200: Loss 0.7134503126144409, R2 0.2395857274532318
Epoch 5: Batch 300: Loss 0.877642810344696, R2 0.23716215789318085
Epoch 5: Batch 400: Loss 0.7783387899398804, R2 0.23327875137329102
Epoch 5: Batch 500: Loss 0.5914472341537476, R2 0.2343376725912094
Training Loss 0.7656957267945103 R2 0.23428687453269958
Val Loss: 0.7582392701331306. R2: 0.2542576193809509
Epoch 6: Batch 0: Loss 0.6438584327697754, R2 0.1444629430770874
Epoch 6: Batch 100: Loss 0.8350598216056824, R2 0.22986683249473572
Epoch 6: Batch 200: Loss 0.6545888185501099, R2 0.23156964778900146
Epoch 6: Batch 300: Loss 0.7699052691459656, R2 0.23269718885421753
Epoch 6: Batch 400: Loss 0.7473467588424683, R2 0.23274099826812744
Epoch 6: Batch 500: Loss 0.973098874092102, R2 0.23226064443588257
Training Loss 0.7664280355636484 R2 0.2335718721151352
Val Loss: 0.755899256643127. R2: 0.2566705346107483
Epoch 7: Batch 0: Loss 0.7234774827957153, R2 0.031674765050411224
Epoch 7: Batch 100: Loss 1.0651767253875732, R2 0.251115620136261
Epoch 7: Batch 200: Loss 0.7729131579399109, R2 0.24777868390083313
Epoch 7: Batch 300: Loss 0.5855552554130554, R2 0.2462036907672882
Epoch 7: Batch 400: Loss 0.762965202331543, R2 0.243916317820549
Epoch 7: Batch 500: Loss 0.6910669803619385, R2 0.24301128089427948
Training Loss 0.7578783003386537 R2 0.24211159348487854
Val Loss: 0.754606715458281. R2: 0.25814539194107056
Epoch 8: Batch 0: Loss 0.6801085472106934, R2 0.33820319175720215
Epoch 8: Batch 100: Loss 0.6955572962760925, R2 0.2623326778411865
Epoch 8: Batch 200: Loss 0.6148346662521362, R2 0.26179832220077515
Epoch 8: Batch 300: Loss 0.9604947566986084, R2 0.2634391188621521
Epoch 8: Batch 400: Loss 0.8530237078666687, R2 0.265424907207489
Epoch 8: Batch 500: Loss 0.6342176198959351, R2 0.2681163549423218
Training Loss 0.7312049668873368 R2 0.26878851652145386
Val Loss: 0.7272422848378911. R2: 0.28484246134757996
Epoch 9: Batch 0: Loss 1.1524436473846436, R2 0.2680485248565674
Epoch 9: Batch 100: Loss 0.9811439514160156, R2 0.27111539244651794
Epoch 9: Batch 200: Loss 0.7630466818809509, R2 0.27530211210250854
Epoch 9: Batch 300: Loss 0.6174472570419312, R2 0.27456414699554443
Epoch 9: Batch 400: Loss 0.6479880809783936, R2 0.27536171674728394
Epoch 9: Batch 500: Loss 0.5207950472831726, R2 0.27685314416885376
Training Loss 0.7237253079115245 R2 0.27627915143966675
Val Loss: 0.7235675692996558. R2: 0.2884758412837982
Epoch 10: Batch 0: Loss 0.6740840673446655, R2 0.3160483241081238
Epoch 10: Batch 100: Loss 0.6421588659286499, R2 0.2798832654953003
Epoch 10: Batch 200: Loss 0.8169329166412354, R2 0.2783530652523041
Epoch 10: Batch 300: Loss 0.6927531957626343, R2 0.2798653841018677
Epoch 10: Batch 400: Loss 0.8296048641204834, R2 0.2803969979286194
Epoch 10: Batch 500: Loss 0.77818363904953, R2 0.28109416365623474
Training Loss 0.7197238029259158 R2 0.28027093410491943
Val Loss: 0.7236871986704714. R2: 0.28835606575012207
Epoch 11: Batch 0: Loss 0.6578598022460938, R2 0.18833701312541962
Epoch 11: Batch 100: Loss 0.6592045426368713, R2 0.2823018431663513
Epoch 11: Batch 200: Loss 0.7991095781326294, R2 0.2809346914291382
Epoch 11: Batch 300: Loss 0.6342475414276123, R2 0.2823430299758911
Epoch 11: Batch 400: Loss 0.5175459384918213, R2 0.28438329696655273
Epoch 11: Batch 500: Loss 0.6116110682487488, R2 0.2830999791622162
Training Loss 0.7165972128015603 R2 0.28340235352516174
Val Loss: 0.7218427013824967. R2: 0.2900673747062683
Epoch 12: Batch 0: Loss 0.8632423281669617, R2 0.278475284576416
Epoch 12: Batch 100: Loss 0.7336556911468506, R2 0.2890729308128357
Epoch 12: Batch 200: Loss 1.1772547960281372, R2 0.2860264778137207
Epoch 12: Batch 300: Loss 0.9149750471115112, R2 0.2847983241081238
Epoch 12: Batch 400: Loss 0.5856393575668335, R2 0.28699004650115967
Epoch 12: Batch 500: Loss 0.48222702741622925, R2 0.28727465867996216
Training Loss 0.7135120961279007 R2 0.28649741411209106
Val Loss: 0.7205970228156623. R2: 0.2913009524345398
Epoch 13: Batch 0: Loss 0.5045638084411621, R2 0.24559877812862396
Epoch 13: Batch 100: Loss 1.0579288005828857, R2 0.2979198098182678
Epoch 13: Batch 200: Loss 0.8835368156433105, R2 0.29153725504875183
Epoch 13: Batch 300: Loss 0.5124468207359314, R2 0.2921156883239746
Epoch 13: Batch 400: Loss 0.5255429148674011, R2 0.2903556525707245
Epoch 13: Batch 500: Loss 0.702354907989502, R2 0.28984344005584717
Training Loss 0.7101940689161694 R2 0.2896806597709656
Val Loss: 0.7201379441163119. R2: 0.2917832136154175
Epoch 14: Batch 0: Loss 0.9203755855560303, R2 0.20315507054328918
Epoch 14: Batch 100: Loss 0.5846381783485413, R2 0.2913562059402466
Epoch 14: Batch 200: Loss 0.8444182276725769, R2 0.29067644476890564
Epoch 14: Batch 300: Loss 0.5609111785888672, R2 0.2941037118434906
Epoch 14: Batch 400: Loss 0.6348145008087158, R2 0.29514485597610474
Epoch 14: Batch 500: Loss 0.6617620587348938, R2 0.29398757219314575
Training Loss 0.7061217056649197 R2 0.293876588344574
Val Loss: 0.7161955443375251. R2: 0.29565665125846863
Epoch 15: Batch 0: Loss 0.6496449708938599, R2 0.374687135219574
Epoch 15: Batch 100: Loss 0.6220478415489197, R2 0.2954956293106079
Epoch 15: Batch 200: Loss 0.7650084495544434, R2 0.297991007566452
Epoch 15: Batch 300: Loss 0.9894622564315796, R2 0.29692715406417847
Epoch 15: Batch 400: Loss 0.5894911289215088, R2 0.29696810245513916
Epoch 15: Batch 500: Loss 0.7965032458305359, R2 0.2980760335922241
Training Loss 0.7012065332314185 R2 0.2987431287765503
Val Loss: 0.7163222274359535. R2: 0.2955409288406372
Epoch 16: Batch 0: Loss 0.7646946907043457, R2 0.2769052982330322
Epoch 16: Batch 100: Loss 0.5260241031646729, R2 0.30438241362571716
Epoch 16: Batch 200: Loss 0.6137287616729736, R2 0.3073868155479431
Epoch 16: Batch 300: Loss 0.7273950576782227, R2 0.30552583932876587
Epoch 16: Batch 400: Loss 0.8937682509422302, R2 0.3021663725376129
Epoch 16: Batch 500: Loss 0.6128377914428711, R2 0.3014618158340454
Training Loss 0.6977749913930893 R2 0.3022216260433197
Val Loss: 0.7166967181598439. R2: 0.2951744496822357
Epoch 17: Batch 0: Loss 0.4242202639579773, R2 0.3279838562011719
Epoch 17: Batch 100: Loss 0.7165311574935913, R2 0.30044645071029663
Epoch 17: Batch 200: Loss 1.0101686716079712, R2 0.2968319058418274
Epoch 17: Batch 300: Loss 0.9878653287887573, R2 0.29554590582847595
Epoch 17: Batch 400: Loss 0.6247983574867249, R2 0.29683151841163635
Epoch 17: Batch 500: Loss 0.6293367147445679, R2 0.2990414798259735
Training Loss 0.6996124104160224 R2 0.3003672957420349
Val Loss: 0.7161372582263806. R2: 0.2956961989402771
Epoch 18: Batch 0: Loss 0.6252238154411316, R2 0.44364458322525024
Epoch 18: Batch 100: Loss 0.7107974886894226, R2 0.30300986766815186
Epoch 18: Batch 200: Loss 0.5987347364425659, R2 0.3088015615940094
Epoch 18: Batch 300: Loss 0.5539781451225281, R2 0.3046203851699829
Epoch 18: Batch 400: Loss 0.639819860458374, R2 0.3040297031402588
Epoch 18: Batch 500: Loss 0.7439365386962891, R2 0.3042036294937134
Training Loss 0.6968701346553999 R2 0.30310940742492676
Val Loss: 0.7168765096541714. R2: 0.2949862480163574
Epoch 19: Batch 0: Loss 0.5373412370681763, R2 0.3930860459804535
Epoch 19: Batch 100: Loss 0.7282065153121948, R2 0.3039446473121643
Epoch 19: Batch 200: Loss 0.7884088754653931, R2 0.30620184540748596
Epoch 19: Batch 300: Loss 1.0286109447479248, R2 0.30479997396469116
Epoch 19: Batch 400: Loss 0.9952744245529175, R2 0.3041718602180481
Epoch 19: Batch 500: Loss 0.6851595640182495, R2 0.303378701210022
Training Loss 0.6980573809894689 R2 0.30186817049980164
Val Loss: 0.7169441863456193. R2: 0.2949719727039337
Epoch 20: Batch 0: Loss 0.8751492500305176, R2 0.2039192169904709
Epoch 20: Batch 100: Loss 0.9256930351257324, R2 0.3012803792953491
Epoch 20: Batch 200: Loss 0.7021965980529785, R2 0.30325958132743835
Epoch 20: Batch 300: Loss 0.7132631540298462, R2 0.30384692549705505
Epoch 20: Batch 400: Loss 0.6716941595077515, R2 0.3036496043205261
Epoch 20: Batch 500: Loss 0.732072114944458, R2 0.303269624710083
Training Loss 0.6976953259363385 R2 0.30230918526649475
Val Loss: 0.7160260256598977. R2: 0.2957964539527893
Epoch 21: Batch 0: Loss 0.8529343605041504, R2 0.2493770569562912
Epoch 21: Batch 100: Loss 1.0872665643692017, R2 0.30334609746932983
Epoch 21: Batch 200: Loss 0.5700781345367432, R2 0.3034285008907318
Epoch 21: Batch 300: Loss 0.8514568209648132, R2 0.3030349612236023
Epoch 21: Batch 400: Loss 0.6539572477340698, R2 0.30293214321136475
Epoch 21: Batch 500: Loss 0.812590479850769, R2 0.30391356348991394
Training Loss 0.6965472363897796 R2 0.30343228578567505
Val Loss: 0.7162940968923709. R2: 0.2955514192581177
Min Val Loss Epoch 20
Final Arrays
[np.float64(0.8312878295284356), np.float64(0.7882880378253345), np.float64(0.777172893725638), np.float64(0.7743430337232857), np.float64(0.7656957267945103), np.float64(0.7664280355636484), np.float64(0.7578783003386537), np.float64(0.7312049668873368), np.float64(0.7237253079115245), np.float64(0.7197238029259158), np.float64(0.7165972128015603), np.float64(0.7135120961279007), np.float64(0.7101940689161694), np.float64(0.7061217056649197), np.float64(0.7012065332314185), np.float64(0.6977749913930893), np.float64(0.6996124104160224), np.float64(0.6968701346553999), np.float64(0.6980573809894689), np.float64(0.6976953259363385), np.float64(0.6965472363897796)]
[0.16870257258415222, 0.21172642707824707, 0.2228032797574997, 0.22566823661327362, 0.23428687453269958, 0.2335718721151352, 0.24211159348487854, 0.26878851652145386, 0.27627915143966675, 0.28027093410491943, 0.28340235352516174, 0.28649741411209106, 0.2896806597709656, 0.293876588344574, 0.2987431287765503, 0.3022216260433197, 0.3003672957420349, 0.30310940742492676, 0.30186817049980164, 0.30230918526649475, 0.30343228578567505]
[np.float64(0.770455589189249), np.float64(0.7809183715897448), np.float64(0.7747903233941864), np.float64(0.7711536577519249), np.float64(0.7582392701331306), np.float64(0.755899256643127), np.float64(0.754606715458281), np.float64(0.7272422848378911), np.float64(0.7235675692996558), np.float64(0.7236871986704714), np.float64(0.7218427013824967), np.float64(0.7205970228156623), np.float64(0.7201379441163119), np.float64(0.7161955443375251), np.float64(0.7163222274359535), np.float64(0.7166967181598439), np.float64(0.7161372582263806), np.float64(0.7168765096541714), np.float64(0.7169441863456193), np.float64(0.7160260256598977), np.float64(0.7162940968923709)]
[0.24236325919628143, 0.23222510516643524, 0.23847660422325134, 0.24196025729179382, 0.2542576193809509, 0.2566705346107483, 0.25814539194107056, 0.28484246134757996, 0.2884758412837982, 0.28835606575012207, 0.2900673747062683, 0.2913009524345398, 0.2917832136154175, 0.29565665125846863, 0.2955409288406372, 0.2951744496822357, 0.2956961989402771, 0.2949862480163574, 0.2949719727039337, 0.2957964539527893, 0.2955514192581177]
